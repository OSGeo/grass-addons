<h2>DESCRIPTION</h2>

<em><b>r.change.info</b></em> calculates landscape change assessment 
for a series of categorical maps, e.g. land cover/land use, with 
different measures based on information theory and machine learning. 
More than two <b>input</b> maps can be specified.

<p>
<em><b>r.change.info</b></em> moves a processing window over the 
<b>input</b> maps. This processing window is the current landscape 
under consideration. The size of the window is defined with 
<b>size</b>. Change assessment is done for each processing window 
(landscape) separately. The centers of the processing windows are 
<b>step</b> cells apart and the <b>output</b> maps will have a 
resolution of <b>step</b> input cells. <b>step</b> should not be larger 
than <b>size</b>, otherwise some cells will be skipped. If <b>step</b> 
is half of <b>size</b> , the moving windows will overlap by 50%. The 
overlap increases when <b>step</b> becomes smaller. A smaller 
<b>step</b> and/or a larger <b>size</b> will require longer processing 
time.

<p>
The measures <em>information gain</em>, <em>information gain 
ratio</em>, <em>CHI-square</em> and <em>Gini-impurity</em> are commonly 
used in decision tree modelling (Quinlan 1986) to compare 
distributions. These measures as well as the statistical distance are 
based on landscape structure and are calculated for the distributions 
of patch categories and/or patch sizes. A patch is a contiguous block 
of cells with the same category (class), for example a forest fragment. 
The proportion of changes is based on cell changes in the current 
landscape.

<h3>Cell-based change assessment</h3>

The method <b>pc</b> calculates the <em>proportion of changes</em> as 
the actual number of cell changes in the current landscape divided by 
the theoretical maximum number of changes (number of cells in the 
processing window x (number of input maps - 1)).

<h3>Landscape structure change assessment</h3>

<h4>Landscape structure</h4>
For each processing window, the number of cells per category are 
counted and patches are identified.  
The size and category of each patch is recorded. From these cell and 
patch statistics, three kinds of patterns (distributions) are 
calculated:
<dl>
<dt><strong>1. Distributions over categories (e.g land cover class)</strong>
<dd>This provides information about changes in categories (e.g land 
cover class), e.g. if one category becomes more prominent. This detects 
changes in category composition.
<dt><strong>2. Distributions over size classes</strong> 
<dd>This provides information about fragmentation, e.g. if a few large 
fragments are broken up into many small fragments. This detects changes 
in fragmentation.
<dt><strong>3. Distributions over categories and size classes.</strong>
<dd>This provides information about whether particular combinations of 
category and size class changed between input maps. This detects 
changes in the general landscape structure.
</dl>

The latter is obtained from the category and size of each patch, i.e. 
each unique category - size class combination becomes a separate class. 
<p>
The numbers indicate which distribtution will be used for the selected 
method (see below).
<p>
A low change in category distributions and a high change in 
size distributions means that the frequency of categories did not 
change much whereas the size of patches did change.

<h4>Information gain</h4>
The methods <b>gain1, gain2 and gain3</b> calculate the <em>information 
gain</em> after Quinlan (1986). The information gain is the difference 
between the entropy of the combined distribution and the average 
entropy of the observed distributions (conditional entropy). A larger 
value means larger differences between input maps.
<p>
Information gain indicates the absolute amount of information gained 
(to be precise, reduced uncertainty) when considering the individual 
input maps instead of their combination. When cells and patches are 
distributed over a large number of categories and a large number of size 
classes, information gain tends to over-estimate changes.
<p>
The information gain can be zero even if all cells changed, but the 
distributions (frequencies of occurrence) remained identical. The square 
root of the information gain is sometimes used as a distance measure 
and it is closely related to Fisher's information metric.

<h4>Information gain ratio</h4>
The methods <b>ratio1, ratio2 and ratio3</b> calculate the <em>information 
gain ratio</em> that changes occurred, estimated with the ratio of the 
average entropy of the observed distributions to the entropy of the 
combined distribution. In other words, the ratio is equivalent to the 
ratio of actual change to maximum possible change (in uncertainty). The 
gain ratio is better suited than absolute information gain when the 
cells are distributed over a large number of categories and a large number 
of size classes. The gain ratio here follows the same rationale as 
the gain ratio of Quinlan (1986), but is calculated differently.
<p>
The gain ratio is always in the range (0, 1). A larger value means 
larger differences between input maps.

<h4>CHI-square</h4>
The methods <b>chisq1, chisq2 and chisq3</b> calculate <em>CHI square</em> 
after Quinlan (1986) to estimate the relevance of the different input 
maps. If the input maps are identical, the relevance measured as 
CHI-square is zero, i.e. no change occurred. If the input maps differ from 
each other substantially, major changes occurred and the relevance 
measured as CHI-square is large.

<h4>Gini impurity</h4>
The methods <b>gini1, gini2 and gini3</b> calculate the <em>Gini 
impurity</em>, which is 1 - Simpson's index, or 1 - 1 / diversity, or 1 
- 1 / 2^entropy for alpha = 1. The Gini impurity can thus be regarded 
as a modified measure of the diversity of a distribution. Changes 
occurred when the diversity of the combined distribution is larger than 
the average diversity of the observed distributions, thus a larger 
value means larger differences between input maps.
<p>
The Gini impurity is always in the range (0, 1) and calculated with<br><br>
G = 1 - <big>&sum;</big> p<sub>i</sub><sup>2</sup>

<p>
The methods <em>information gain</em> and <em>CHI square</em> are the 
most sensitive measures, but also the most susceptible to noise. The 
<em>information gain ratio</em> is less sensitive, but more robust 
against noise. The <em>Gini impurity</em> is the least sensitive and 
detects only drastic changes.

<h4>Distance</h4>
The methods <b>dist1, dist2 and dist3</b> calculate the statistical 
<em>distance</em> from the absolute differences between the average 
distribution and the observed distributions. The distance is always in 
the range (0, 1). A larger value means larger differences between input 
maps.

<p>
Methods using the category or size class distributions (<em>gain1</em>, 
<em>gain2</em>, <em>ratio1</em>, <em>ratio2</em> <em>gini1</em>, 
<em>gini2</em>, <em>dist1</em>, <em>dist2</em>) are less sensitive than 
methods using the combined category and size class distributions 
(<em>gain3</em>, <em>ratio3</em>, <em>gini3</em>, <em>dist3</em>).

<p>
For a thorough change assessment it is recommended to calculate 
different change assessment measures (at least information gain and 
information gain ratio) and investigate the differences between these 
change assessments.

<h2>NOTES</h2>
<h3>Shannon's entropy</h3>
Entropies for information gain and its ratio are by default Shannon 
entropies <em>H</em>, calculated with<br><br>
H = <big>&sum;</big> p<sub>i</sub> * log<sub>2</sub>(p<sub>i</sub>)

<p>
The entropies are here calculated with base 2 logarithms. The upper 
bound of information gain is thus log<sub>2</sub>(number of classes). 
Classes can be categories, size classes, or unique combinations of 
categories and size classes.

<h3>R&eacute;nyi's entropy</h3>
The <b>alpha</b> option can be used to calculate general entropies 
<em>H<sub>&alpha;</sub></em> after R&eacute;nyi (1961) with the formula<br><br>
H<sub>&alpha;</sub> = 1 / (1 - &alpha;) * log<sub>2</sub> 
(<big>&sum;</big> p<sub>i</sub><sup>&alpha;</sup>)

<p>
An <b>alpha</b> &lt; 1 gives higher weight to small frequencies, 
whereas an <b>alpha</b> &gt; 1 gives higher weight to large 
frequencies. This is useful for noisy input data such as the MODIS land 
cover/land use products MCD12*. These data often differ only in 
single-cell patches. These differences can be due to the applied 
classification procedure. Moreover, the probabilities that a cell has 
been assigned to class A or class B are often very similar, i.e. 
different classes are confused by the applied classification procedure. 
In such cases an <b>alpha</b> &gt; 1, e.g. 2, will give less weight to 
small changes and more weight to large changes, to a degree alleviating 
the problem of class confusion.

<h2>EXAMPLES</h2>

Assuming there is a time series of the MODIS land cover/land use 
product MCD12Q1, land cover type 1, available, and the raster maps have 
the names
<div class="code"><pre>
MCD12Q1.A2001.Land_Cover_Type_1
MCD12Q1.A2002.Land_Cover_Type_1
MCD12Q1.A2003.Land_Cover_Type_1
...
</pre></div>

then a change assessment can be done with
<div class="code"><pre>
r.change.info in=`g.list type=rast pat=MCD12Q1.A*.Land_Cover_Type_1 sep=,` \
	      method=pc,gain1,gain2,ratio1,ratio2,dist1,dist2 
	      out=MCD12Q1.pc,MCD12Q1.gain1,MCD12Q1.gain2,MCD12Q1.ratio1,MCD12Q1.ratio2,MCD12Q1.dist1,MCD12Q1.dist2 \
	      radius=20 step=40 alpha=2
</pre></div>

<h2>SEE ALSO</h2>

<em><a href="r.neighbors.html">r.neighbors</a></em><br>
<em><a href="g.region.html">g.region</a></em><br>
<em><a href="r.li.shannon.html">r.li.shannon</a></em><br>
<em><a href="r.li.simpson.html">r.li.simpson</a></em><br>
<em><a href="r.li.renyi.html">r.li.renyi</a></em><br>
<em><a href="r.clump.html">r.clump</a></em>

<h2>REFERENCES</h2>

<ul>
<li>
Quinlan, J.R. 1986. Induction of decision trees. Machine Learning 1: 81-106.
<a href="http://dx.doi.org/10.1007/BF00116251">DOI:10.1007/BF00116251</a>
<li>
R&eacute;nyi, A. 1961. <a href="http://digitalassets.lib.berkeley.edu/math/ucb/text/math_s4_v1_article-27.pdf">On measures of information and entropy.</a> Proceedings of the fourth Berkeley Symposium on Mathematics, Statistics and Probability 1960: 547-561.
<li>
Shannon, C.E. 1948. A Mathematical Theory of Communication. Bell System Technical Journal 27(3): 379-423. <a href="http://dx.doi.org/10.1002/j.1538-7305.1948.tb01338.x">DOI:10.1002/j.1538-7305.1948.tb01338.x</a>
</ul>

<h2>AUTHOR</h2>

Markus Metz 

<!--
<p>
<i>Last changed: $Date$</i>
-->
  
