#!/usr/bin/env python
############################################################################
# MODULE:        r.learn.ml
# AUTHOR:        Steven Pawley
# PURPOSE:       Supervised classification and regression of GRASS rasters
#                using the python scikit-learn package
#
# COPYRIGHT: (c) 2016 Steven Pawley, and the GRASS Development Team
#                This program is free software under the GNU General Public
#                for details.
#
#############################################################################

#%module
#% description: Supervised classification and regression of GRASS rasters using the python scikit-learn package
#% keyword: classification
#% keyword: regression
#% keyword: machine learning
#% keyword: scikit-learn
#%end

#%option G_OPT_I_GROUP
#% key: group
#% label: Imagery group to be classified
#% description: Series of raster maps to be used in the random forest classification
#% required: yes
#% multiple: no
#%end

#%option G_OPT_R_INPUT
#% key: trainingmap
#% label: Labelled pixels
#% description: Raster map with labelled pixels for training
#% required: no
#% guisection: Required
#%end

#%option G_OPT_R_OUTPUT
#% key: output
#% required: yes
#% label: Output Map
#% description: Prediction surface result from classification or regression model
#%end

#%option string
#% key: classifier
#% required: yes
#% label: Classifier
#% description: Supervised learning model to use
#% answer: RandomForestClassifier
#% options: LogisticRegression,LinearDiscriminantAnalysis,QuadraticDiscriminantAnalysis,GaussianNB,DecisionTreeClassifier,DecisionTreeRegressor,RandomForestClassifier,RandomForestRegressor,GradientBoostingClassifier,GradientBoostingRegressor,SVC,EarthClassifier,EarthRegressor
#%end

#%option
#% key: c
#% type: double
#% description: Inverse of regularization strength (logistic regresson and SVC)
#% answer: 1.0
#% multiple: yes
#% guisection: Classifier Parameters
#%end

#%option
#% key: max_features
#% type: integer
#% description: Number of features to consider during splitting for tree based classifiers. Default is sqrt(n_features) for classification, and n_features for regression
#% required: no
#% answer:
#% multiple: yes
#% guisection: Classifier Parameters
#%end

#%option
#% key: max_depth
#% type: integer
#% description: Optionally specifiy maximum tree depth. Otherwise full-growing occurs for decision trees and random forests, and max_depth=3 for gradient boosting
#% required: no
#% answer:
#% multiple: yes
#% guisection: Classifier Parameters
#%end

#%option
#% key: min_samples_split
#% type: double
#% description: The minimum number of samples required for node splitting in tree based classifiers
#% answer: 2
#% multiple: yes
#% guisection: Classifier Parameters
#%end

#%option
#% key: min_samples_leaf
#% type: integer
#% description: The minimum number of samples required to form a leaf node for tree based classifiers
#% answer: 1
#% multiple: yes
#% guisection: Classifier Parameters
#%end

#%option
#% key: n_estimators
#% type: integer
#% description: Number of estimators for tree-based classifiers
#% answer: 100
#% multiple: yes
#% guisection: Classifier Parameters
#%end

#%option
#% key: learning_rate
#% type: double
#% description: learning rate for gradient boosting
#% answer: 0.1
#% multiple: yes
#% guisection: Classifier Parameters
#%end

#%option
#% key: subsample
#% type: double
#% description: The fraction of samples to be used for fitting for gradient boosting
#% answer: 1.0
#% multiple: yes
#% guisection: Classifier Parameters
#%end

#%option integer
#% key: max_degree
#% description: The maximum degree of terms generated by the forward pass in Earth
#% answer: 1
#% multiple: yes
#% guisection: Classifier Parameters
#%end

# General options

#%flag
#% key: s
#% label: Standardization preprocessing
#% guisection: Optional
#%end

#%option string
#% key: categorymaps
#% required: no
#% multiple: yes
#% label: Indices of categorical rasters within the imagery group (0..n)
#% description: Indices of categorical rasters within the imagery group (0..n)
#%end

#%option string
#% key: cvtype
#% required: no
#% label: Non-spatial or spatial cross-validation
#% description: Non-spatial, clumped or clustered k-fold cross-validation
#% answer: Non-spatial
#% options: non-spatial,clumped,kmeans
#%end

#%option
#% key: n_partitions
#% type: integer
#% description: Number of kmeans spatial partitions
#% answer: 10
#% guisection: Optional
#%end

#%option G_OPT_R_INPUT
#% key: group_raster
#% label: Custom group ids for labelled pixels from GRASS raster
#% description: GRASS raster containing group ids for labelled pixels
#% required: no
#% guisection: Optional
#%end

#%option
#% key: cv
#% type: integer
#% description: Number of cross-validation folds for performance evaluation
#% answer: 1
#% guisection: Optional
#%end

#%option
#% key: random_state
#% type: integer
#% description: Seed to pass onto the random state for reproducible results
#% answer: 1
#% guisection: Optional
#%end

#%option G_OPT_F_OUTPUT
#% key: errors_file
#% label: Save cross-validation global accuracy results to csv
#% required: no
#% guisection: Optional
#%end

#%option
#% key: lines
#% type: integer
#% description: Processing block size in terms of number of rows
#% answer: 25
#% guisection: Optional
#%end

#%flag
#% key: p
#% label: Output class membership probabilities
#% guisection: Optional
#%end

#%flag
#% key: m
#% description: Build model only - do not perform prediction
#% guisection: Optional
#%end

#%flag
#% key: f
#% description: Calculate feature importances using permutation
#% guisection: Optional
#%end

#%option
#% key: n_iter
#% type: integer
#% description: Number of randomized parameter tuning steps
#% answer: 1
#% guisection: Optional
#%end

#%option
#% key: tune_cv
#% type: integer
#% description: Number of cross-validation folds used for parameter tuning
#% answer: 3
#% guisection: Optional
#%end

#%option
#% key: n_permutations
#% type: integer
#% description: Number of permutations to perform for feature importances
#% answer: 10
#% guisection: Optional
#%end

#%option G_OPT_F_OUTPUT
#% key: fimp_file
#% label: Save feature importances to csv
#% required: no
#% guisection: Optional
#%end

#%flag
#% key: b
#% description: Balance training data by random oversampling
#% guisection: Optional
#%end

#%option G_OPT_F_OUTPUT
#% key: save_training
#% label: Save training data to csv
#% required: no
#% guisection: Optional
#%end

#%option G_OPT_F_INPUT
#% key: load_training
#% label: Load training data from csv
#% required: no
#% guisection: Optional
#%end

#%option G_OPT_F_OUTPUT
#% key: save_model
#% label: Save model from file
#% required: no
#% guisection: Optional
#%end

#%option G_OPT_F_INPUT
#% key: load_model
#% label: Load model from file
#% required: no
#% guisection: Optional
#%end

#%flag
#% key: l
#% label: Use memory swap
#% guisection: Optional
#%end

#%rules
#% exclusive: trainingmap,load_model
#% exclusive: load_training,save_training

#%end

import atexit
import os
import numpy as np
import copy
import grass.script as grass
import tempfile
from grass.pygrass.modules.shortcuts import imagery as im
from grass.pygrass.modules.shortcuts import raster as r
from subprocess import PIPE
from grass.pygrass.raster import RasterRow
from grass.pygrass.gis.region import Region
from grass.pygrass.raster.buffer import Buffer


class train():

    def __init__(self, estimator, X, y, groups=None, categorical_var=None,
                 standardize=False, balance=False):
        """
        Train class to perform preprocessing, fitting, parameter search and
        cross-validation in a single step

        Args
        ----
        estimator: Scikit-learn compatible estimator object
        X, y: training data and labels as numpy arrays
        groups: groups to be used for cross-validation
        categorical_var: 1D list containing indices of categorical predictors
        standardize: Transform predictors
        balance: boolean to balance number of classes
        """

        # fitting data
        self.estimator = estimator
        self.X = X
        self.y = y
        self.groups = groups
        self.balance = balance

        # for onehot-encoding
        self.enc = None
        self.categorical_var = categorical_var
        self.category_values = None
        
        if self.categorical_var:
            self.onehotencode()
        
        # for standardization
        if standardize == True:
            self.standardization()
        else:
            self.scaler = None

        # for cross-validation scores
        self.scores = None
        self.scores_cm = None
        self.fimp = None


    def random_oversampling(self, X, y, random_state=None):
        """
        Balances X, y observations using simple oversampling
        
        Args
        ----
        X: numpy array of training data
        y: 1D numpy array of response data
        random_state: Seed to pass onto random number generator
        
        Returns
        -------
        X_resampled: Numpy array of resampled training data
        y_resampled: Numpy array of resampled response data
        """
        
        np.random.seed(seed=random_state)
        
        # count the number of observations per class
        y_classes = np.unique(y)
        class_counts = np.histogram(y, bins=len(y_classes))[0]
        maj_counts = class_counts.max()
 
        y_resampled = y
        X_resampled = X
        
        for cla, counts in zip(y_classes, class_counts):
            # get the number of samples needed to balance minority class
            num_samples = maj_counts - counts
            
            # get the indices of the ith class
            indx = np.nonzero(y==cla)
            
            # create some new indices         
            oversamp_indx = np.random.choice(indx[0], size=num_samples)
    
            # concatenate to the original X and y
            y_resampled = np.concatenate((y[oversamp_indx], y_resampled))
            X_resampled = np.concatenate((X[oversamp_indx], X_resampled))
            
            return (X_resampled, y_resampled)


    def onehotencode(self):
        """
        Method to convert a list of categorical arrays in X into a suite of
        binary predictors which are added to the left of the array
        """

        from sklearn.preprocessing import OneHotEncoder

        # store original range of values
        self.category_values = [0] * len(self.categorical_var)
        for i, cat in enumerate(self.categorical_var):
            self.category_values[i] = np.unique(self.X[:, cat])
        
        # fit and transform categorical grids to a suite of binary features
        self.enc = OneHotEncoder(categorical_features=self.categorical_var,
                                 sparse=False)
        self.enc.fit(self.X)
        self.X = self.enc.transform(self.X)    


    def fit(self, param_distributions=None, param_grid=None, n_iter=3, cv=3,
            random_state=None):

        """
        Main fit method for the train object. Performs fitting, hyperparameter
        search and cross-validation in one step (inspired from R's CARET)

        Args
        ----
        param_distributions: continuous parameter distribution to be used in a 
        randomizedCVsearch
        param_grid: Dist of non-continuous parameters to grid search
        n_iter: Number of randomized search iterations
        cv: Number of cross-validation folds for parameter tuning
        random_state: seed to be used during random number generation
        """

        from sklearn.model_selection import RandomizedSearchCV, GridSearchCV
        from sklearn.model_selection import GroupKFold
        
        # Balance classes
        if self.balance == True:
            X, y = self.random_oversampling(self.X, self.y, random_state=random_state)
            
            if self.groups is not None:
                groups, _ = self.random_oversampling(
                    self.groups, self.y, random_state=random_state)
            else:
                groups = None
        else:
            X = self.X
            y = self.y
            groups = self.groups

        # Randomized or grid search
        if param_distributions is not None or param_grid is not None:
            
            # use groupkfold for hyperparameter search if groups are present
            if self.groups is not None:
                cv_search = GroupKFold(n_splits=cv)
            else:
                cv_search = cv
        
            # Randomized search
            if param_distributions is not None:
                self.estimator = RandomizedSearchCV(
                    estimator=self.estimator,
                    param_distributions=param_distributions,
                    n_iter=n_iter,
                    cv=cv_search)
            
            # Grid Search
            if param_grid is not None:
                self.estimator = GridSearchCV(self.estimator,
                                              param_grid,
                                              n_jobs=-1, cv=cv_search)
                        
            # if groups then fit RandomizedSearchCV.fit requires groups param
            if self.groups is None:
                self.estimator.fit(X, y)
            else:
                self.estimator.fit(X, y, groups=groups)
        
        # Fitting without parameter search
        else:
            self.estimator.fit(X, y)


    def standardization(self):
        """
        Transforms the non-categorical X
        """

        from sklearn.preprocessing import StandardScaler
        
        # create mask so that indices that represent categorical
        # predictors are not selected
        if self.categorical_var is not None:
            idx = np.arange(self.X.shape[1])
            mask = np.ones(len(idx), dtype=bool)
            mask[self.categorical_var] = False
        else:
            mask = np.arange(self.X.shape[1])

        X_continuous = self.X[:, mask]    
        self.scaler = StandardScaler()
        self.scaler.fit(X_continuous)
        self.X[:, mask] =  self.scaler.transform(X_continuous)


    def pred_func(self, estimator, X_test, y_true, scorers):
        """
        Calculates a single performance metric depending on if scorer type
        is binary, multiclass or regression

        To be called from the varImp_permutation

        Args
        ----
        estimator: fitted estimator on training set
        X_test: Test training data
        y_true: Test labelled data
        scorers: String indicating which type of scorer to be used
        """

        from sklearn import metrics

        if scorers == 'binary':
            scorer = metrics.roc_auc_score
            y_pred = estimator.predict_proba(X_test)[:, 1]
        if scorers == 'multiclass':
            scorer = metrics.accuracy_score
            y_pred = estimator.predict(X_test)
        if scorers == 'regression':
            scorer = metrics.r2_score
            y_pred = estimator.predict(X_test)

        score = scorer(y_true, y_pred)

        return (score)


    def varImp_permutation(self, estimator, X_test, y_true,
                           n_permutations, scorers,
                           random_state):

        """
        Method to perform permutation-based feature importance during
        cross-validation (cross-validation is applied externally to this
        method)

        Procedure is:
        1. Pass fitted estimator and test partition X y
        2. Assess AUC on the test partition (bestauc)
        3. Permute each variable and assess the difference between bestauc and
           the messed-up variable
        4. Repeat (3) for many random permutations
        5. Average the repeats

        Args
        ----
        estimator: estimator that has been fitted to a training partition
        X_test, y_true: data and labels from a test partition
        n_permutations: number of random permutations to apply
        random_state: seed to pass to the numpy random.seed

        Returns
        -------
        scores: AUC scores for each predictor following permutation
        """

        # calculate score on original variables without permutation
        # determine best metric type for binary/multiclass/regression scenarios
        best_score = self.pred_func(estimator, X_test, y_true, scorers)

        np.random.seed(seed=random_state)
        scores = np.zeros((n_permutations, X_test.shape[1]))

        # outer loop to repeat the pemutation rep times
        for rep in range(n_permutations):

            # inner loop to permute each predictor variable and assess
            # difference in auc
            for i in range(X_test.shape[1]):
                Xscram = np.copy(X_test)
                Xscram[:, i] = np.random.choice(X_test[:, i], X_test.shape[0])

                # fit the model on the training data and predict the test data
                scores[rep, i] = best_score-self.pred_func(
                    estimator, Xscram, y_true, scorers)
                if scores[rep, i] < 0: scores[rep, i] = 0

        # average the repetitions
        scores = scores.mean(axis=0)

        return(scores)


    def specificity_score(self, y_true, y_pred):

        from sklearn.metrics import confusion_matrix

        cm = confusion_matrix(y_true, y_pred)

        tn = float(cm[0][0])
        #fn = float(cm[1][0])
        #tp = float(cm[1][1])
        fp = float(cm[0][1])

        specificity = tn/(tn+fp)

        return (specificity)


    def cross_val(self, scorers='binary', cv=3, feature_importances=False,
                  n_permutations=25, random_state=None):

        from sklearn.model_selection import StratifiedKFold
        from sklearn.model_selection import GroupKFold
        from sklearn.model_selection import RandomizedSearchCV
        from sklearn import metrics

        """
        Stratified Kfold and GroupFold cross-validation
        Generates suites of scoring_metrics for binary classification,

        multiclass classification and regression scenarios

        Args
        ----
        scorers: Suite of performance metrics to use
        cv: Integer of cross-validation folds
        feature_importances: Boolean to perform permutation-based importances
        n_permutations: Number of permutations during feature importance
        random_state: Seed to pass to the random number generator
        """

        # dictionary of lists to store metrics
        if scorers == 'binary':
            self.scores = {
                'accuracy': [],
                'precision': [],
                'recall': [],
                'specificity': [],
                'f1': [],
                'kappa': [],
                'auc': []
            }

        if scorers == 'multiclass':
            self.scores = {
                'accuracy': [],
                'f1': [],
                'kappa': []
            }

        if scorers == 'regression':
            self.scores = {
                'r2': []
            }

        y_test_agg = []
        y_pred_agg = []
        self.fimp = np.zeros((cv, self.X.shape[1]))

        # generate Kfold indices
        if self.groups is None:
            k_fold = StratifiedKFold(
                n_splits=cv,
                shuffle=False,
                random_state=random_state).split(self.X, self.y)
        else:
            k_fold = GroupKFold(n_splits=cv).split(
                self.X, self.y, groups=self.groups)

        for train_indices, test_indices in k_fold:

            # get indices for train and test partitions
            X_train, X_test = self.X[train_indices], self.X[test_indices]
            y_train, y_test = self.y[train_indices], self.y[test_indices]         
            
            # also get indices of groups for the training partition
            if self.groups is not None:
                groups_train = self.groups[train_indices]

            # balance the fold
            if self.balance == True:
                X_train, y_train = self.random_oversampling(X_train, y_train, random_state=random_state)                
                if self.groups is not None:
                    groups_train, _ = self.random_oversampling(
                        groups_train, y_train, random_state=random_state) 
                
            # fit the model on the training data and predict the test data
            # need the groups parameter because the estimator can be a 
            # RandomizedSearchCV estimator where cv=GroupKFold
            if self.groups is not None and isinstance(self.estimator, RandomizedSearchCV):
                fit = self.estimator.fit(X_train, y_train, groups=groups_train)            
            else:
                fit = self.estimator.fit(X_train, y_train)   

            y_pred = fit.predict(X_test)

            y_test_agg = np.append(y_test_agg, y_test)
            y_pred_agg = np.append(y_pred_agg, y_pred)

            labels = np.unique(y_pred)

            # calculate metrics
            if scorers == 'binary':
                self.scores['accuracy'] = np.append(
                    self.scores['accuracy'],
                    metrics.accuracy_score(y_test, y_pred))

                y_pred_proba = fit.predict_proba(X_test)[:, 1]
                self.scores['auc'] = np.append(
                    self.scores['auc'],
                    metrics.roc_auc_score(y_test, y_pred_proba))

                self.scores['precision'] = np.append(
                    self.scores['precision'], metrics.precision_score(
                        y_test, y_pred, labels, average='binary'))

                self.scores['recall'] = np.append(
                    self.scores['recall'], metrics.recall_score(
                        y_test, y_pred, labels, average='binary'))

                self.scores['specificity'] = np.append(
                    self.scores['specificity'], self.specificity_score(
                        y_test, y_pred))

                self.scores['f1'] = np.append(
                    self.scores['f1'], metrics.f1_score(
                        y_test, y_pred, labels, average='binary'))

                self.scores['kappa'] = np.append(
                    self.scores['kappa'],
                    metrics.cohen_kappa_score(y_test, y_pred))

            elif scorers == 'multiclass':

                self.scores['accuracy'] = np.append(
                    self.scores['accuracy'],
                    metrics.accuracy_score(y_test, y_pred))

                self.scores['kappa'] = np.append(
                    self.scores['kappa'],
                    metrics.cohen_kappa_score(y_test, y_pred))

            elif scorers == 'regression':
                self.scores['r2'] = np.append(
                    self.scores['r2'], metrics.r2_score(y_test, y_pred))

            # feature importances using permutation
            if feature_importances == True:
                if (self.fimp==0).all() == True:
                    self.fimp = self.varImp_permutation(
                        fit, X_test, y_test, n_permutations, scorers,
                        random_state)
                else:
                    self.fimp = np.row_stack(
                        (self.fimp, self.varImp_permutation(
                            fit, X_test, y_test,
                            n_permutations, scorers, random_state)))

        self.scores_cm = metrics.classification_report(y_test_agg, y_pred_agg)

        # convert onehot-encoded feature importances back to original vars
        if self.fimp is not None and self.enc is not None:
            
            from copy import deepcopy

            # get start,end positions of each suite of onehot-encoded vars
            feature_ranges = deepcopy(self.enc.feature_indices_)
            for i in range(0, len(self.enc.feature_indices_)-1):
                feature_ranges[i+1] = feature_ranges[i] + len(self.category_values[i])
            
            # take sum of each onehot-encoded feature
            ohe_feature = [0] * len(self.categorical_var)
            ohe_sum = [0] * len(self.categorical_var)
            
            for i in range(len(self.categorical_var)):
                ohe_feature[i] = self.fimp[:, feature_ranges[i]:feature_ranges[i+1]]
                ohe_sum[i] = ohe_feature[i].sum(axis=1)
                
            # remove onehot-encoded features from the importances array
            features_for_removal = np.array(range(feature_ranges[-1]))
            self.fimp = np.delete(self.fimp, features_for_removal, axis=1)      
            
            # insert summed importances into original positions
            for index in self.categorical_var:
                self.fimp = np.insert(self.fimp, np.array(index), ohe_sum[0], axis=1)


    def predict(self, predictors, output, class_probabilities=False,
               rowincr=25):

        """
        Prediction on list of GRASS rasters using a fitted scikit learn model

        Parameters
        ----------
        predictors: List of GRASS rasters

        class_probabilties: Predict class probabilities

        rowincr: Integer of raster rows to process at one time
        output: Name of GRASS raster to output classification results
        """

        # determine output data type and nodata
        predicted = self.estimator.predict(self.X)

        if (predicted % 1 == 0).all() == True:
            ftype = 'CELL'
            nodata = -2147483648
        else:
            ftype = 'FCELL'
            nodata = np.nan

        # create a list of rasterrow objects for predictors
        n_features = len(predictors)
        rasstack = [0] * n_features

        for i in range(n_features):
            rasstack[i] = RasterRow(predictors[i])
            if rasstack[i].exist() is True:
                rasstack[i].open('r')
            else:
                grass.fatal("GRASS raster " + predictors[i] +
                            " does not exist.... exiting")

        # use grass.pygrass.gis.region to get information about the current region
        current = Region()

        # create a imagery mask
        # the input rasters might have different dimensions and non-value pixels.
        # r.series used to automatically create a mask by propagating the nulls
        grass.run_command("r.series", output='tmp_clfmask',
                          input=predictors, method='count', flags='n',
                          overwrite=True)

        mask_raster = RasterRow('tmp_clfmask')
        mask_raster.open('r')

        # create and open RasterRow objects for classification
        classification = RasterRow(output)
        classification.open('w', ftype, overwrite=True)

        # create and open RasterRow objects for  probabilities if enabled
        if class_probabilities is True:

            # determine number of classes
            labels = np.unique(self.y)
            nclasses = len(labels)

            prob_out_raster = [0] * nclasses
            prob = [0] * nclasses

            for iclass in range(nclasses):
                prob_out_raster[iclass] = output + \
                    '_classPr' + str(int(labels[iclass]))
                prob[iclass] = RasterRow(prob_out_raster[iclass])
                prob[iclass].open('w', 'FCELL', overwrite=True)

        """
        Prediction using row blocks
        """

        for rowblock in range(0, current.rows, rowincr):

            # check that the row increment does not exceed the number of rows
            if rowblock+rowincr > current.rows:
                rowincr = current.rows - rowblock
            img_np_row = np.zeros((rowincr, current.cols, n_features))
            mask_np_row = np.zeros((rowincr, current.cols))

            # loop through each row, and each band
            # and add these values to the 2D array img_np_row
            for row in range(rowblock, rowblock+rowincr, 1):
                mask_np_row[row-rowblock, :] = np.array(mask_raster[row])

                for band in range(n_features):
                    img_np_row[row-rowblock, :, band] = \
                        np.array(rasstack[band][row])

            mask_np_row[mask_np_row == -2147483648] = np.nan
            nanmask = np.isnan(mask_np_row)  # True in mask means invalid data
            
            # reshape each row-band matrix into a n*m array
            nsamples = rowincr * current.cols
            flat_pixels = img_np_row.reshape((nsamples, n_features))

            # remove NaN values and GRASS CELL nodata vals
            flat_pixels[flat_pixels == -2147483648] = np.nan
            flat_pixels = np.nan_to_num(flat_pixels)

            # onehot-encoding
            if self.enc is not None:
                try:
                    flat_pixels = self.enc.transform(flat_pixels)
                except:
                    # if this fails it is because the onehot-encoder was fitted
                    # on the training samples, but the prediction data contains
                    # new values, i.e. the training data has not sampled all of
                    # categories
                    grass.fatal('There are values in the categorical rasters that are not present in the training data set, i.e. the training data has not sampled all of the categories')
            
            # rescale
            if self.scaler is not None:
                # create mask so that indices that represent categorical
                # predictors are not selected
                if self.categorical_var is not None:
                    idx = np.arange(self.X.shape[1])
                    mask = np.ones(len(idx), dtype=bool)
                    mask[self.categorical_var] = False
                else:
                    mask = np.arange(self.X.shape[1])
                flat_pixels_continuous = flat_pixels[:, mask]        
                flat_pixels[:, mask] =  self.scaler.transform(flat_pixels_continuous)

            # perform prediction
            result = self.estimator.predict(flat_pixels)
            result = result.reshape((rowincr, current.cols))

            # replace NaN values so that the prediction does not have a border
            result = np.ma.masked_array(
                result, mask=nanmask, fill_value=-99999)

            # return a copy of result, with masked values filled with a value
            result = result.filled([nodata])

            # for each row we can perform computation, and write the result
            for row in range(rowincr):
                newrow = Buffer((result.shape[1],), mtype=ftype)
                newrow[:] = result[row, :]
                classification.put_row(newrow)

            # same for probabilities
            if class_probabilities is True:
                result_proba = self.estimator.predict_proba(flat_pixels)

                for iclass in range(result_proba.shape[1]):

                    result_proba_class = result_proba[:, iclass]
                    result_proba_class = result_proba_class.reshape(
                                            (rowincr, current.cols))

                    result_proba_class = np.ma.masked_array(
                        result_proba_class, mask=nanmask, fill_value=np.nan)

                    result_proba_class = result_proba_class.filled([np.nan])

                    for row in range(rowincr):

                        newrow = Buffer((
                                    result_proba_class.shape[1],),
                                    mtype='FCELL')

                        newrow[:] = result_proba_class[row, :]
                        prob[iclass].put_row(newrow)

        # close all maps
        for i in range(n_features):
            rasstack[i].close()

        classification.close()
        mask_raster.close()

        try:
            for iclass in range(nclasses):
                prob[iclass].close()
        except:
            pass


def cleanup():
    grass.run_command("g.remove", name='tmp_clfmask',
                      flags="f", type="raster", quiet=True)
    grass.run_command("g.remove", name='tmp_roi_clumped',
              flags="f", type="raster", quiet=True)


def model_classifiers(estimator='LogisticRegression', random_state=None,
                      C=1, max_depth=None, max_features='auto',
                      min_samples_split=2, min_samples_leaf=1,
                      n_estimators=100, subsample=1.0,
                      learning_rate=0.1, max_degree=1):

    """
    Provides the classifiers and parameters using by the module

    Args
    ----
    estimator: Name of estimator
    random_state: Seed to use in randomized components
    C: Inverse of regularization strength
    max_depth: Maximum depth for tree-based methods
    min_samples_split: Minimum number of samples to split a node
    min_samples_leaf: Minimum number of samples to form a leaf
    n_estimators: Number of trees
    subsample: Controls randomization in gradient boosting
    learning_rate: Used in gradient boosting
    max_degree: For earth

    Returns
    -------
    clf: Scikit-learn classifier object
    mode: Flag to indicate whether classifier performs classification or
          regression
    """

    from sklearn.linear_model import LogisticRegression
    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
    from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
    from sklearn.naive_bayes import GaussianNB
    from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
    from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
    from sklearn.ensemble import GradientBoostingClassifier
    from sklearn.ensemble import GradientBoostingRegressor
    from sklearn.svm import SVC

    # optional packages that add additional classifiers here
    if estimator == 'EarthClassifier' or estimator == 'EarthRegressor':
        try:
            from sklearn.pipeline import Pipeline
            from pyearth import Earth

            # Combine Earth with LogisticRegression in a pipeline to do classification
            earth_classifier = Pipeline([('Earth',
                Earth(max_degree=max_degree)), ('Logistic', LogisticRegression())])

            classifiers = {'EarthClassifier': earth_classifier,
                           'EarthRegressor': Earth(max_degree=max_degree)}
        except:
            grass.fatal('Py-earth package not installed')
    else:
        # core sklearn classifiers go here
        classifiers = {
            'SVC': SVC(C=C, probability=True, random_state=random_state),
            'LogisticRegression':
                LogisticRegression(C=C, random_state=random_state, n_jobs=-1),
            'DecisionTreeClassifier':
                DecisionTreeClassifier(max_depth=max_depth,
                                      max_features=max_features,
                                      min_samples_split=min_samples_split,
                                      min_samples_leaf=min_samples_leaf,
                                      random_state=random_state),
            'DecisionTreeRegressor':
                DecisionTreeRegressor(max_features=max_features,
                                      min_samples_split=min_samples_split,
                                      min_samples_leaf=min_samples_leaf,
                                      random_state=random_state),
            'RandomForestClassifier':
                RandomForestClassifier(n_estimators=n_estimators,
                                       max_features=max_features,
                                       min_samples_split=min_samples_split,
                                       min_samples_leaf=min_samples_leaf,
                                       random_state=random_state,
                                       n_jobs=-1,
                                       oob_score=False),
            'RandomForestRegressor':
                RandomForestRegressor(n_estimators=n_estimators,
                                      max_features=max_features,
                                      min_samples_split=min_samples_split,
                                      min_samples_leaf=min_samples_leaf,
                                      random_state=random_state,
                                      n_jobs=-1,
                                      oob_score=False),
            'GradientBoostingClassifier':
                GradientBoostingClassifier(learning_rate=learning_rate,
                                           n_estimators=n_estimators,
                                           max_depth=max_depth,
                                           min_samples_split=min_samples_split,
                                           min_samples_leaf=min_samples_leaf,
                                           subsample=subsample,
                                           max_features=max_features,
                                           random_state=random_state),
            'GradientBoostingRegressor':
                GradientBoostingRegressor(learning_rate=learning_rate,
                                          n_estimators=n_estimators,
                                          max_depth=max_depth,
                                          min_samples_split=min_samples_split,
                                          min_samples_leaf=min_samples_leaf,
                                          subsample=subsample,
                                          max_features=max_features,
                                          random_state=random_state),
            'GaussianNB': GaussianNB(),
            'LinearDiscriminantAnalysis': LinearDiscriminantAnalysis(),
            'QuadraticDiscriminantAnalysis': QuadraticDiscriminantAnalysis(),
        }

    # define classifier
    clf = classifiers[estimator]

    # classification or regression
    if estimator == 'LogisticRegression' \
        or estimator == 'DecisionTreeClassifier' \
        or estimator == 'RandomForestClassifier' \
        or estimator == 'GradientBoostingClassifier' \
        or estimator == 'GaussianNB' \
        or estimator == 'LinearDiscriminantAnalysis' \
        or estimator == 'QuadraticDiscriminantAnalysis' \
        or estimator == 'EarthClassifier' \
            or estimator == 'SVC':
            mode = 'classification'
    else:
        mode = 'regression'

    return (clf, mode)


def save_training_data(X, y, groups, file):

    """
    Saves any extracted training data to a csv file

    Args
    ----
    X: Numpy array containing predictor values
    y: Numpy array containing labels
    groups: Numpy array of group labels
    file: Path to a csv file to save data to
    """

    # if there are no group labels, create a nan filled array
    if groups is None:
        groups = np.empty((y.shape[0]))
        groups[:] = np.nan

    training_data = np.column_stack([X, y, groups])
    np.savetxt(file, training_data, delimiter=',')


def load_training_data(file):

    """
    Loads training data and labels from a csv file

    Args
    ----
    file: Path to a csv file to save data to

    Returns
    -------
    X: Numpy array containing predictor values
    y: Numpy array containing labels
    groups: Numpy array of group labels, or None
    """

    training_data = np.loadtxt(file, delimiter=',')
    n_cols = training_data.shape[1]
    last_Xcol = n_cols-2

    # check to see if last column contains group labels or nans
    groups = training_data[:, -1]

    # if all nans then set groups to None
    if np.isnan(groups).all() == True:
        groups = None

    # fetch X and y
    X = training_data[:, 0:last_Xcol]
    y = training_data[:, -2]

    return(X, y, groups)


def sample_predictors(response, predictors, shuffle_data=True, lowmem=False,
                      random_state=None):

    from sklearn.utils import shuffle

    """
    Samples a list of GRASS rasters using a labelled raster
    Per raster sampling

    Args
    ----
    response: String; GRASS raster with labelled pixels
    predictors: List of GRASS rasters containing explanatory variables

    Returns
    -------

    training_data: Numpy array of extracted raster values
    training_labels: Numpy array of labels
    y_indexes: Row and Columns of label positions
    """

    current = Region()

    # open response raster as rasterrow and read as np array
    if RasterRow(response).exist() is True:
        roi_gr = RasterRow(response)
        roi_gr.open('r')

        if lowmem is False:
            response_np = np.array(roi_gr)
        else:
            response_np = np.memmap(tempfile.NamedTemporaryFile(),
                                    dtype='float32', mode='w+',
                                    shape=(current.rows, current.cols))
            response_np[:] = np.array(roi_gr)[:]
    else:
        grass.fatal("GRASS response raster does not exist.... exiting")

    # determine number of predictor rasters
    n_features = len(predictors)

    # check to see if all predictors exist
    for i in range(n_features):
        if RasterRow(predictors[i]).exist() is not True:
            grass.fatal("GRASS raster " + predictors[i] +
                        " does not exist.... exiting")

    # check if any of those pixels are labelled (not equal to nodata)
    # can use even if roi is FCELL because nodata will be nan
    is_train = np.nonzero(response_np > -2147483648)
    training_labels = response_np[is_train]
    n_labels = np.array(is_train).shape[1]

    # Create a zero numpy array of len training labels
    if lowmem is False:
        training_data = np.zeros((n_labels, n_features))
    else:
        training_data = np.memmap(tempfile.NamedTemporaryFile(),
                                  dtype='float32', mode='w+',
                                  shape=(n_labels, n_features))

    # Loop through each raster and sample pixel values at training indexes
    if lowmem is True:
        feature_np = np.memmap(tempfile.NamedTemporaryFile(),
					   dtype='float32', mode='w+',
                               shape=(current.rows, current.cols))

    for f in range(n_features):
        predictor_gr = RasterRow(predictors[f])
        predictor_gr.open('r')

        if lowmem is False:
            feature_np = np.array(predictor_gr)
        else:
            feature_np[:] = np.array(predictor_gr)[:]

        training_data[0:n_labels, f] = feature_np[is_train]

        # close each predictor map
        predictor_gr.close()

    # convert any CELL maps no datavals to NaN in the training data
    for i in range(n_features):
        training_data[training_data[:, i] == -2147483648] = np.nan

    # convert indexes of training pixels from tuple to n*2 np array
    is_train = np.array(is_train).T

    # Remove nan rows from training data
    X = training_data[~np.isnan(training_data).any(axis=1)]
    y = training_labels[~np.isnan(training_data).any(axis=1)]
    y_indexes = is_train[~np.isnan(training_data).any(axis=1)]

    # shuffle the training data
    if shuffle_data is True:
        X, y, y_indexes = shuffle(X, y, y_indexes, random_state=random_state)

    # close the response map
    roi_gr.close()

    return(X, y, y_indexes)


def sample_training_data(response, maplist, group_raster='', n_partitions=3,
                         cvtype='', lowmem=False, random_state=None):

    """
    Samples predictor and optional group id raster for cross-val

    Args
    ----
    roi: String; GRASS raster with labelled pixels
    maplist: List of GRASS rasters containing explanatory variables
    group_raster: GRASS raster containing group ids of labelled pixels
    n_partitions: Number of spatial partitions
    cvtype: Type of spatial clustering
    save_training: Save extracted training data to .csv file
    lowmem: Boolean to use numpy memmap during extraction
    random_state: Seed

    Returns
    -------
    X: Numpy array of extracted raster values
    y: Numpy array of labels
    group_id: Group ids of labels
    """

    from sklearn.cluster import KMeans

    # clump the labelled pixel raster if labels represent polygons
    # then set the group_raster to the clumped raster to extract the group_ids
    # used in the GroupKFold cross-validation
    # ------------------------------------------------------------------------
    if cvtype == 'clumped' and group_raster == '':
        r.clump(input=response, output='tmp_roi_clumped',
                overwrite=True, quiet=True)
        group_raster = 'tmp_roi_clumped'

    # extract training data from maplist and take group ids from
    # group_raster. Shuffle=False so that group ids and labels align
    # because cross-validation will be performed spatially
    # ---------------------------------------------------------------
    if group_raster != '':
        maplist2 = copy.deepcopy(maplist)
        maplist2.append(group_raster)
        X, y, sample_coords = sample_predictors(response=response,
                                                predictors=maplist2,
                                                shuffle_data=False,
                                                lowmem=False,
                                                random_state=random_state)
        # take group id from last column and remove column from predictors
        group_id = X[:, -1]
        X = np.delete(X, -1, axis=1)

        # remove the clumped raster
        try:
            grass.run_command(
                "g.remove", name='tmp_roi_clumped', flags="f",
                type="raster", quiet=True)
        except:
            pass

    # extract training data from maplist without group Ids
    # shuffle this data by default
    # ----------------------------------------------------
    else:
        X, y, sample_coords = sample_predictors(
            response=response, predictors=maplist,
            shuffle_data=True,
            lowmem=lowmem,
            random_state=random_state)

        group_id = None

        if cvtype == 'kmeans':
            clusters = KMeans(n_clusters=n_partitions,
                              random_state=random_state,
                              n_jobs=-1)

            clusters.fit(sample_coords)
            group_id = clusters.labels_

    return (X, y, group_id)


def maps_from_group(group):
    """
    Parse individual rasters into a list from an imagery group

    Args
    ----
    group: String; GRASS imagery group
    Returns
    -------
    maplist: Python list containing individual GRASS raster maps
    """
    groupmaps = im.group(group=group, flags="g",
                         quiet=True, stdout_=PIPE).outputs.stdout

    maplist = groupmaps.split(os.linesep)
    maplist = maplist[0:len(maplist)-1]
    map_names = []

    for rastername in maplist:
        map_names.append(rastername.split('@')[0])

    return(maplist, map_names)


def main():

    try:
        from sklearn.externals import joblib

    except:
        grass.fatal("Scikit learn 0.18 or newer is not installed")

    """
    GRASS options and flags
    -----------------------
    """

    # General options and flags
    group = options['group']
    response = options['trainingmap']
    output = options['output']
    classifier = options['classifier']
    norm_data = flags['s']
    cv = int(options['cv'])
    cvtype = options['cvtype']
    group_raster = options['group_raster']
    categorymaps = options['categorymaps']    
    n_partitions = int(options['n_partitions'])
    modelonly = flags['m']
    probability = flags['p']
    rowincr = int(options['lines'])
    random_state = int(options['random_state'])
    model_save = options['save_model']
    model_load = options['load_model']
    load_training = options['load_training']
    save_training = options['save_training']
    importances = flags['f']
    tune_cv = int(options['tune_cv'])
    n_permutations = int(options['n_permutations'])
    lowmem = flags['l']
    errors_file = options['errors_file']
    fimp_file = options['fimp_file']
    balance = flags['b']
   
    if ',' in categorymaps:
        categorymaps = [int(i) for i in categorymaps.split(',')]
    else:
        categorymaps = None
        
    param_grid = {'C': None,
                'min_samples_split': None,
                'min_samples_leaf': None,
                'n_estimators': None,
                'learning_rate': None,
                'subsample': None,
                'max_depth': None,
                'max_features': None,
                'max_degree': None}
    
    # classifier options
    C = options['c']
    if ',' in C:
        param_grid['C'] = [float(i) for i in C.split(',')]
        C = None
    else:
        C = float(C)
    
    min_samples_split = options['min_samples_split']
    if ',' in min_samples_split:
        param_grid['min_samples_split'] = [float(i) for i in min_samples_split.split(',')]
        min_samples_split = None                
    else:
        min_samples_split = int(min_samples_split)
    
    min_samples_leaf = options['min_samples_leaf']
    if ',' in min_samples_leaf:
        param_grid['min_samples_leaf'] = [int(i) for i in min_samples_leaf.split(',')]
        min_samples_leaf = None
    else:
        min_samples_leaf = int(min_samples_leaf)

    n_estimators = options['n_estimators']
    if ',' in n_estimators:
        param_grid['n_estimators'] = [int(i) for i in n_estimators.split(',')]
        n_estimators = None
    else:
        n_estimators = int(n_estimators)

    learning_rate = options['learning_rate']
    if ',' in learning_rate:
        param_grid['learning_rate'] = [float(i) for i in learning_rate.split(',')]
        learning_rate = None
    else:
        learning_rate = float(learning_rate)

    subsample = options['subsample']
    if ',' in subsample:
        param_grid['subsample'] = [float(i) for i in subsample.split(',')]
        subsample = None
    else:
        subsample = float(subsample)

    max_depth = options['max_depth']
    if max_depth == '':
        max_depth = None
    else:
        if ',' in max_depth:
            param_grid['max_depth'] = [int(i) for i in max_depth.split(',')]
            max_depth = None
        else:
            max_depth = float(max_depth)
    
    max_features = options['max_features']
    if max_features == '':
        max_features = 'auto'
    else:
        if ',' in max_features:
            param_grid['max_features'] = [int(i) for i in max_features.split(',')]
            max_features = None
        else:
            max_features = int(max_features)
    
    max_degree = options['max_degree']
    if ',' in max_degree:
        param_grid['max_degree'] = [int(i) for i in max_degree.split(',')]
        max_degree = None
    else:
        max_degree = int(max_degree)
    
    if importances is True and cv == 1:
        grass.fatal('Feature importances require cross-validation cv > 1')

    # fetch individual raster names from group
    maplist, map_names = maps_from_group(group)

    """
    Train the classifier
    --------------------
    """

    # Sample training data and group ids
    # Perform parameter tuning and cross-validation
    # Unless a previously fitted model is to be loaded
    # ------------------------------------------------
    if model_load == '':

        # Sample training data and group id
        if load_training != '':
            X, y, group_id = load_training_data(load_training)
        else:
            X, y, group_id = sample_training_data(
                response, maplist, group_raster, n_partitions, cvtype,
                lowmem, random_state)

        # option to save extracted data to .csv file
        if save_training != '':
            save_training_data(X, y, group_id, save_training)

        # retrieve sklearn classifier object and parameters
        grass.message("Classifier = " + classifier)

        clf, mode = \
            model_classifiers(classifier, random_state,
                              C, max_depth, max_features, min_samples_split,
                              min_samples_leaf, n_estimators,
                              subsample, learning_rate, max_degree)
        
        # turn off balancing if mode = regression
        if mode == 'regression' and balance == True:
            balance = False

        # remove empty items from the param_grid dict
        param_grid = {k: v for k, v in param_grid.iteritems() if v != None}
        
        # check that dict keys are compatible for the selected classifier
        clf_params = clf.get_params()
        param_grid = { key: value for key, value in param_grid.iteritems() if key in clf_params}
        
        # check if dict contains and keys, otherwise set it to None
        # so that the train object will not perform GridSearchCV
        if any(param_grid) != True: param_grid = None
        
        # Decide on scoring metric scheme
        if mode == 'classification':
            if len(np.unique(y)) == 2 and all([0, 1] == np.unique(y)):
                scorers = 'binary'
            else:
                scorers = 'multiclass'
        else:
            scorers = 'regression'
        
        if mode == 'regression' and probability is True:
            grass.warning(
                'Class probabilities only valid for classifications...ignoring')
            probability = False

        # create training object - onehot-encoded on-the-fly
        learn_m = train(clf, X, y, group_id, categorical_var=categorymaps,
                        standardize=norm_data, balance=balance)

        """
        Fitting, parameter search and cross-validation
        ----------------
        """

        # fit and parameter search
        learn_m.fit(param_grid=param_grid, cv=tune_cv, random_state=random_state)

        if param_grid is not None:
            grass.message('\n')
            grass.message('Best parameters:')
            grass.message(str(learn_m.estimator.best_params_))

        # If cv > 1 then use cross-validation to generate performance measures
        if cv > 1:
            grass.message('\r\n')
            grass.message(
                "Cross validation global performance measures......:")
            
            # cross-validate the training object
            learn_m.cross_val(scorers, cv, importances, n_permutations=n_permutations,
                              random_state=random_state)

            if mode == 'classification':
                if scorers == 'binary':
                    grass.message(
                        "Accuracy   :\t%0.3f\t+/-SD\t%0.3f" %
                        (learn_m.scores['accuracy'].mean(),
                         learn_m.scores['accuracy'].std()))
                    grass.message(
                        "AUC        :\t%0.3f\t+/-SD\t%0.3f" %
                        (learn_m.scores['auc'].mean(),
                         learn_m.scores['auc'].std()))
                    grass.message(
                        "Kappa      :\t%0.3f\t+/-SD\t%0.3f" %
                        (learn_m.scores['kappa'].mean(),
                         learn_m.scores['kappa'].std()))
                    grass.message(
                        "Precision  :\t%0.3f\t+/-SD\t%0.3f" %
                        (learn_m.scores['precision'].mean(),
                         learn_m.scores['precision'].std()))
                    grass.message(
                        "Recall     :\t%0.3f\t+/-SD\t%0.3f" %
                        (learn_m.scores['recall'].mean(),
                         learn_m.scores['recall'].std()))
                    grass.message(
                        "Specificity:\t%0.3f\t+/-SD\t%0.3f" %
                        (learn_m.scores['specificity'].mean(),
                         learn_m.scores['specificity'].std()))
                    grass.message(
                        "F1         :\t%0.3f\t+/-SD\t%0.3f" %
                        (learn_m.scores['f1'].mean(),
                         learn_m.scores['f1'].std()))

                if scorers == 'multiclass':
                    grass.message(
                        "Accuracy:\t%0.3f\t+/-SD\t%0.3f" %
                        (learn_m.scores['accuracy'].mean(),
                         learn_m.scores['accuracy'].std()))
                    grass.message(
                        "Kappa   :\t%0.3f\t+/-SD\t%0.3f" %
                        (learn_m.scores['kappa'].mean(),
                         learn_m.scores['kappa'].std()))

                # classification report
                grass.message("\n")
                grass.message("Classification report:")
                grass.message(learn_m.scores_cm)

            else:
                grass.message("R2:\t%0.3f\t+/-\t%0.3f" %
                              (learn_m.scores['r2'].mean(),
                               learn_m.scores['r2'].std()))

            # write cross-validation results for csv file
            if errors_file != '':
                try:
                    import pandas as pd
                    errors = pd.DataFrame(learn_m.scores)
                    errors.to_csv(errors_file, mode='w')
                except:
                    grass.warning("Pandas is not installed. Pandas is required to write the cross-validation results to file")

            # feature importances
            if importances is True:
                grass.message("\r\n")
                grass.message("Feature importances")
                grass.message("id" + "\t" + "Raster" + "\t" + "Importance")

                # mean of cross-validation feature importances
                for i in range(len(learn_m.fimp.mean(axis=0))):
                    grass.message(
                        str(i) + "\t" + maplist[i] +
                        "\t" + str(round(learn_m.fimp.mean(axis=0)[i], 4)))

                if fimp_file != '':
                    np.savetxt(fname=fimp_file, X=learn_m.fimp, delimiter=',',
                               header=','.join(maplist), comments='')
    else:
        # load a previously fitted train object
        # -------------------------------------
        if model_load != '':
            # load a previously fitted model
            learn_m = joblib.load(model_load)

    """
    Optionally save the fitted model
    ---------------------
    """

    if model_save != '':
        joblib.dump(learn_m, model_save)


    """
    Prediction on the rest of the GRASS rasters in the imagery group
    ----------------------------------------------------------------
    """
    if modelonly is not True:
        learn_m.predict(maplist, output, probability, rowincr)
    else:
        grass.message("Model built and now exiting")

if __name__ == "__main__":
    options, flags = grass.parser()
    atexit.register(cleanup)
    main()
