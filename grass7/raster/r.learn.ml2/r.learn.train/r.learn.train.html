<h2>DESCRIPTION</h2>

<p><em>r.learn.train</em> performs training data extraction, supervised machine learning and cross validation using the
	python package <em>scikit learn</em>. The choice of machine learning algorithm is set using the
	<em>model_name</em> parameter. For more details relating to the classifiers, refer to the <a
		href="http://scikit-learn.org/stable/">scikit learn documentation.</a>. The training data can be provided by
	a GRASS raster map containing labelled pixels using the <em>training_map</em> parameter, or can come from a GRASS
	vector dataset containing point geometries using the <em>training_points</em> parameter. If a vector map is
	used then the <em>field</em> parameter also needs to indicate which column in the vector
	attribute table contains the labels/values for training. For regression models this field must contain only
	numeric values. For classification models the field can contain integer-encoded labels, or it can represent text, in which
	case the text categories will be encoded as numeric values (in alphabetical order). These text labels will
	also be applied as categories to the classification output when using <b>r.learn.predict</b>. The vector map should
	also not contain multiple geometries per attribute.</p>

<h3>Supervised Learning Algorithms</h3>

<p>The following classification and regression methods are available:</p>

<ul>
	<li><em>LogisticRegression</em> and <em>LinearRegression</em> represent linear models for classification and
		regression respectively. The <em>SGDClassifier</em> and <em>SGDRegressor</em> are also linear models for
		classification and regression, but they use stochastic gradient descent optimization suitable for large
		datasets and supports l1, l2 and elastic net regularization.</li>

	<li><em>LinearDiscriminantAnalysis</em> and <em>QuadraticDiscriminantAnalysis</em> are classifiers with linear and
		quadratic decision surfaces. These classifiers do not take any parameters and are inherently multiclass. They
		can only be used for classification.</li>

	<li><em>KNeighborsClassifier</em> and <em>KNeighborsRegressor</em> are local approximation methods that assign
		predictions to new observations based on the values assigned to the k-nearest observations in the training data
		feature space. Two hyperparameters are exposed, with <em>n_neighbors</em> governing the number of neighbors to
		used to decide the prediction label, and <em>weights</em> specifying whether these neighbors should have equal
		weights or whether they should be inversely weighted by their distance.</li>

	<li><em>GaussianNB</em> is the Gaussian Naive Bayes algorithm and can be used for classification only. Naive Bayes
		is a supervised learning algorithm based on applying Bayes theorem with the naive assumption of independence
		between every pair of features. This classifier does not take any parameters.</li>

	<li>The <em>DecisionTreeClassifier</em> and <em>DecisionTreeRegressor</em> map observations to a response variable
		using a hierarchy of splits and branches. The terminus of these branches, termed leaves, represent the
		prediction of the response variable. Decision trees are non-parametric and can model non-linear relationships
		between a response and predictor variables, and are insensitive the scaling of the predictors.</li>

	<li><em>RandomForestClassifier</em> and <em>RandomForestRegressor</em> represent ensemble classification and
		regression tree methods. Random forests overcome some of the disadvantages of single decision trees by
		constructing an ensemble of uncorrelated trees. Each tree is constructed from a random subsample of the training
		data and only a random subset of the predictors based on <em>max_features</em> is made available during each
		node split. Each tree produces a prediction probability and the final classification result is obtained by
		averaging of the prediction probabilities across all of the trees. The <em>ExtraTreesClassifier</em> and
		<em>ExtraTreesRegressor</em> are variant on random forests where during each node split, the splitting rule that
		is selected is based on the best of a collection of randomly-geneated thresholds that were assigned to the
		features.</li>

	<li><em>GradientBoostingClassifier</em> and <em>GradientBoostingRegressor</em> represent ensemble
		tree-based methods. However, in a boosted model the learning processes is additive in a forward step-wise
		fashion whereby <i>n_estimators</i> are fit during each model step, and each model step is designed to better
		fit samples that are not currently well predicted by the previous step. This incrementally improves the
		performance of the entire model ensemble by fitting to the model residuals. The
		<em>HistGradientBoostingClassifier</em> and <em>HistGradientBoostingRegressor</em> use the new scikit learn
		multi-threaded implementations.</li>

	<li><em>SVC</em> and <em>SVR</em> represent Support Vector Machine classifiers and regressors. Only a linear kernel
		is enabled in <em>r.learn.ml</em> because the use of non-linear kernels are too slow for most remote sensing and
		spatial analysis datasets, which consist of large numbers of samples.</li>

	<li><em>MLPClassifier</em> and <em>MLPRegressor</em> provide access to a multi-layer perceptron algorithm for
		classification or regression.</li>
</ul>

<h3>Hyperparameters</h3>

<p>The Estimator settings tab provides access to the most pertinent parameters that affect the previously described
	algorithms. The scikit-learn estimator defaults are generally supplied, and these parameters can be tuned
	using a grid-search by inputting multiple comma-separated parameters. The grid search is performed
	using a 2-fold cross validation. This tuning can also be accomplished simultaneously with nested cross-validation by
	settings the <em>cv</em> option to &gt 1. The parameters consist of:</p>

<ul>
	<li><em>alpha</em> is the constant used to multiply the regularization term for the <em>SGDClassifier</em> and
		<em>SGDRegressor</em> algorithm, as well as the <em>MLPClassifier</em> and <em>MLPRegressor</em>.</li>

	<li><em>l1_ratio</em> represents the elastic net mixing ratio between l1 and l2 regularization. Applies to
		the <em>SGDClassifier</em> and <em>SGDRegressor</em> options.</li>

	<li><em>c</em> is the inverse of the regularization strength, which is when a penalty is applied to avoid
		overfitting. <em>c</em> applies to the <em>LogisticRegression</em> and <em>SVC</em> and <em>SVR</em> models.
	</li>

	<li><em>epsilon</em> represents the width of the margin that is used to maximize the number of fitted
		observations within. Applies only to the <em>SVR</em> model.</li>

	<li><em>n_estimators</em> represents the number of trees in Random Forest and Extra Trees models, and the number of
		trees used in
		each model step during Gradient Boosting. For random forests, a larger number of trees will never adversely
		affect accuracy although this is at the expensive of computational performance. In contrast, Gradient boosting
		will start to overfit if <em>n_estimators</em> is too high, which will reduce model accuracy.</li>

	<li><em>max_features</em> controls the number of variables that are allowed to be chosen from at each node split in
		the ensemble tree-based models (random forests and gradient boosting), and can be considered to control the
		degree of correlation between the trees.</li>

	<li><em>min_samples_leaf</em> controls the number of samples required to form a leaf node in the tree-based models.</li>

	<li><em>learning_rate</em> and <em>subsample</em> apply only to gradient boosting algorithms. <em>learning_rate</em>
		shrinks the contribution of each tree, and <em>subsample</em> is the
		fraction of randomly selected samples for each tree. A lower <em>learning_rate</em> always improves accuracy in
		gradient boosting but will require a much larger <em>n_estimators</em> setting which will lower computational
		performance.</li>

	<li><em>hidden_units</em> specifies the number of neurons within each hidden layer when using the
		<em>MLPClassifier</em>
		or <em>MLPRegressor</em> estimators. The syntax for this parameter is (100;100) for 100 neurons in two hidden
		layers,
		or for example (300;100;50) for 300 neurons in the first hidden layer, 100 neurons in the second layer, and
		50 neurons in the third. Hyperparameter tuning of the number of hidden layers and the number of neurons in
		each layer can be performed by specifying multiple comma-separated values, e.g. (100;100),(200;200).</li>
</ul>

<p>The following table summarizes the hyperparameter and which models they apply to:</p>

<table style="width:90%">
	<tr>
		<th>Hyperparameter</th>
		<th>Description</th>
		<th>Method</th>
	</tr>
	<tr>
		<td>alpha</td>
		<td>The constrant used to multiply the regularization term</td>
		<td>SGDClassifier, SGDRegressor, MLPClassifier, MLPRegressor</td>
	</tr>
	<tr>
		<td>l1_ratio</td>
		<td>The elastic net mixing ration between l1 and l2 regularization</td>
		<td>SGDClassifier, SGDRegressor</td>
	</tr>
	<tr>
		<td>c</td>
		<td>Inverse of the regularization strength</td>
		<td>LogisticRegression, SVC, SVR</td>
	</tr>
	<tr>
		<td>epsilon</td>
		<td>Width of the margin used to maximize the number of fitted observations</td>
		<td>SVR</td>
	</tr>
	<tr>
		<td>n_estimators</td>
		<td>The number of trees</td>
		<td>RandomForestClassifier, RandomForestRegressor, ExtraTreesClassifier, ExtraTreesRegressor,
			GradientBoostingClassifier, GradientBoostingRegressor, HistGradientBoostingClassifier,
			HistGradientBoostingRegressor</td>
	</tr>
	<tr>
		<td>max_features</td>
		<td>The number of predictor variables that are randomly selected to be available at each node split</td>
		<td>RandomForestClassifier, RandomForestRegressor, ExtraTreesClassifier, ExtraTreesRegressor,
			GradientBoostingClassifier, GradientBoostingRegressor, HistGradientBoostingClassifier,
			HistGradientBoostingRegressor</td>
	</tr>
	<tr>
		<td>min_samples_leaf</td>
		<td>The number of samples required to split a node</td>
		<td>RandomForestClassifier, RandomForestRegressor, ExtraTreesClassifier, ExtraTreesRegressor,
			GradientBoostingClassifier, GradientBoostingRegressor, HistGradientBoostingClassifier,
			HistGradientBoostingRegressor</td>
	</tr>
	<tr>
		<td>learning_rate</td>
		<td>Shrinkage parameter to control the contribution of each tree</td>
		<td>GradientBoostingClassifier, GradientBoostingRegressor, HistGradientBoostingClassifier,
			HistGradientBoostingRegressor</td>
	</tr>
	<tr>
		<td>hidden_units</td>
		<td>The number of neurons in each hidden layer</td>
		<td>MLPClassifier, MLRRegressor</td>
	</tr>
</table>

<h3>Preprocessing</h3>

<p>Although tree-based classifiers are insensitive to the scaling of the input data, other classifiers such as linear
	models may not perform optimally if some predictors have variances that are orders of magnitude larger than others.
	The <em>-s</em> flag adds a standardization preprocessing step to the classification and prediction to reduce this
	effect. Additionally, most of the classifiers do not perform well if there is a large class imbalance in the
	training data. Using the <em>-b</em> flag balances the training data by weighting of the minority classes relative
	to the majority class. This does not apply to the Naive Bayes or LinearDiscriminantAnalysis classifiers.</p>

<p>Scikit learn does not specifically recognize raster predictors that represent non-ordinal, categorical
	values, for example if using a landcover map as a predictor. Predictive performances may be improved if the
	categories in these maps are one-hot encoded before training. The parameter <em>categorical_maps</em> can be used to
	select rasters that in contained within the imagery group to apply one-hot encoding before training.</p>

<h3>Feature Importances</h3>

<p>In addition to model fitting and prediction, feature importances can be generated using the <b>-f</b> flag. The
	feature importances method uses a permutation-based method can be applied to all the estimators. The feature
	importances represent the average decrease in performance of each variable when permuted. For binary
	classifications, the AUC is used as the metric. Multiclass classifications use accuracy, and regressions use R2.</p>

<h3>Cross Validation</h3>

<p>Cross validation can be performed by setting the <em>cv</em> parameters to &gt 1. Cross-validation is performed using
	stratified k-folds for classification and k-folds for regression. Several global and per-class accuracy measures are
	produced depending on whether the response variable is binary or multiclass, or the classifier is for regression or
	classification. Cross-validation can also be performed in groups by supplying a raster containing the group_ids of
	the partitions using the <em>group_raster</em> option. In this case, training samples with the same group id as set
	by the group_raster will never be split between training and test partitions during cross-validation. This can
	reduce problems with overly optimistic cross-validation scores if the training data are strongly spatially
	correlated, i.e. the training data represent rasterized polygons.</p>

<h2>NOTES</h2>

<p>Many of the estimators involve a random process which can causes a small amount of variation in the
	classification/regression results and and feature importances. To enable reproducible results, a seed is supplied to
	the estimator. This can be changed using the <em>randst</em> parameter.</p>

<p>For convenience when repeatedly training models on the same data, the training data can be saved to a csv file using
	the <em>save_training</em> option. This data can then imported into subsequent classification runs, saving time by
	avoiding the need to repeatedly query the predictors.</p>

<h2>EXAMPLE</h2>

<p>Here we are going to use the GRASS GIS sample North Carolina data set as a basis to perform a landsat classification.
	We are going to classify a Landsat 7 scene from 2000, using training information from an older (1996) land cover
	dataset.</p>

<p>Landsat 7 (2000) bands 7,4,2 color composite example:</p>
<center>
	<img src="lsat7_2000_b742.png" alt="Landsat 7 (2000) bands 7,4,2 color composite example">
</center>

<p>Note that this example must be run in the "landsat" mapset of the North Carolina sample data set location.</p>

<p>First, we are going to generate some training pixels from an older (1996) land cover classification:</p>
<div class="code">
	<pre>
		g.region raster=landclass96 -p
		r.random input=landclass96 npoints=1000 raster=training_pixels
</pre>
</div>

<p>Then we can use these training pixels to perform a classification on the more recently obtained landsat 7 image:</p>
<div class="code">
	<pre>
		# train a random forest classification model using r.learn.train 
		r.learn.train group=lsat7_2000 training_map=training_pixels \
			model_name=RandomForestClassifier n_estimators=500 save_model=rf_model.gz

		# perform prediction using r.learn.predict
		r.learn.predict group=lsat7_2000 load_model=rf_model.gz output=rf_classification

		# check raster categories - they are automatically applied to the classification output
		r.category rf_classification

		# copy color scheme from landclass training map to result
		r.colors rf_classification raster=training_pixels
</pre>
</div>

<p>Random forest classification result:</p>
<center>
	<img src="rfclassification.png" alt="Random forest classification result">
</center>

<h2>REFERENCES</h2>

<p>Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011.</p>

<h2>AUTHOR</h2>

Steven Pawley