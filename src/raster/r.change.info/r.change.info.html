<h2>DESCRIPTION</h2>

<em><b>r.change.info</b></em> calculates landscape change assessment
for a series of categorical maps, e.g. land cover/land use, with
different measures based on information theory and machine learning.
More than two <b>input</b> maps can be specified.

<p>
<em><b>r.change.info</b></em> moves a processing window over the
<b>input</b> maps. This processing window is the current landscape
under consideration. The size of the window is defined with
<b>size</b>. Change assessment is done for each processing window
(landscape) separately. The centers of the processing windows are
<b>step</b> cells apart and the <b>output</b> maps will have a
resolution of <b>step</b> input cells. <b>step</b> should not be larger
than <b>size</b>, otherwise some cells will be skipped. If <b>step</b>
is half of <b>size</b> , the moving windows will overlap by 50%. The
overlap increases when <b>step</b> becomes smaller. A smaller
<b>step</b> and/or a larger <b>size</b> will require longer processing
time.

<p>
The measures <em>information gain</em>, <em>information gain
ratio</em>, <em>CHI-square</em> and <em>Gini-impurity</em> are commonly
used in decision tree modelling (Quinlan 1986) to compare
distributions. These measures as well as the statistical distance are
based on landscape structure and are calculated for the distributions
of patch categories and/or patch sizes. A patch is a contiguous block
of cells with the same category (class), for example a forest fragment.
The proportion of changes is based on cell changes in the current
landscape.

<h3>Cell-based change assessment</h3>

The method <b>pc</b> calculates the <em>proportion of changes</em> as
the actual number of cell changes in the current landscape divided by
the theoretical maximum number of changes (number of cells in the
processing window x (number of input maps - 1)).

<h3>Landscape structure change assessment</h3>

<h4>Landscape structure</h4>
For each processing window, the number of cells per category are
counted and patches are identified.
The size and category of each patch are recorded. From these cell and
patch statistics, three kinds of patterns (distributions) are
calculated:
<dl>
<dt><strong>1. Distributions over categories (e.g land cover class)</strong>
<dd>This provides information about changes in categories (e.g land
cover class), e.g. if one category becomes more prominent. This detects
changes in category composition.
<dt><strong>2. Distributions over size classes</strong>
<dd>This provides information about fragmentation, e.g. if a few large
fragments are broken up into many small fragments. This detects changes
in fragmentation.
<dt><strong>3. Distributions over categories and size classes.</strong>
<dd>This provides information about whether particular combinations of
category and size class changed between input maps. This detects
changes in the general landscape structure.
</dl>

The latter is obtained from the category and size of each patch, i.e.
each unique category - size class combination becomes a separate class.
<p>
The numbers indicate which distribution will be used for the selected
method (see below).
<p>
A low change in category distributions and a high change in
size distributions means that the frequency of categories did not
change much whereas the size of patches did change.

<h4>Information gain</h4>
The methods <b>gain1, gain2 and gain3</b> calculate the <em>information
gain</em> after Quinlan (1986). The information gain is the difference
between the entropy of the combined distribution and the average
entropy of the observed distributions (conditional entropy). A larger
value means larger differences between input maps.
<p>
Information gain indicates the absolute amount of information gained
(to be precise, reduced uncertainty) when considering the individual
input maps instead of their combination. When cells and patches are
distributed over a large number of categories and a large number of size
classes, information gain tends to over-estimate changes.
<p>
The information gain can be zero even if all cells changed, but the
distributions (frequencies of occurrence) remained identical. The square
root of the information gain is sometimes used as a distance measure
and it is closely related to Fisher's information metric.

<h4>Information gain ratio</h4>
The methods <b>ratio1, ratio2 and ratio3</b> calculate the <em>information
gain ratio</em> that changes occurred, estimated with the ratio of the
average entropy of the observed distributions to the entropy of the
combined distribution. In other words, the ratio is equivalent to the
ratio of actual change to maximum possible change (in uncertainty). The
gain ratio is better suited than absolute information gain when the
cells are distributed over a large number of categories and a large number
of size classes. The gain ratio here follows the same rationale as
the gain ratio of Quinlan (1986), but is calculated differently.
<p>
The gain ratio is always in the range (0, 1). A larger value means
larger differences between input maps.

<h4>CHI-square</h4>
The methods <b>chisq1, chisq2 and chisq3</b> calculate <em>CHI square</em>
after Quinlan (1986) to estimate the relevance of the different input
maps. If the input maps are identical, the relevance measured as
CHI-square is zero, i.e. no change occurred. If the input maps differ from
each other substantially, major changes occurred and the relevance
measured as CHI-square is large.

<h4>Gini impurity</h4>
The methods <b>gini1, gini2 and gini3</b> calculate the <em>Gini
impurity</em>, which is 1 - Simpson's index, or 1 - 1 / diversity, or 1
- 1 / 2^entropy for alpha = 1. The Gini impurity can thus be regarded
as a modified measure of the diversity of a distribution. Changes
occurred when the diversity of the combined distribution is larger than
the average diversity of the observed distributions, thus a larger
value means larger differences between input maps.
<p>
The Gini impurity is always in the range (0, 1) and calculated with<br><br>
G = 1 - <big>&sum;</big> p<sub>i</sub><sup>2</sup>

<p>
The methods <em>information gain</em> and <em>CHI square</em> are the
most sensitive measures, but also the most susceptible to noise. The
<em>information gain ratio</em> is less sensitive, but more robust
against noise. The <em>Gini impurity</em> is the least sensitive and
detects only drastic changes.

<h4>Distance</h4>
The methods <b>dist1, dist2 and dist3</b> calculate the statistical
<em>distance</em> from the absolute differences between the average
distribution and the observed distributions. The distance is always in
the range (0, 1). A larger value means larger differences between input
maps.

<p>
Methods using the category or size class distributions (<em>gain1</em>,
<em>gain2</em>, <em>ratio1</em>, <em>ratio2</em> <em>gini1</em>,
<em>gini2</em>, <em>dist1</em>, <em>dist2</em>) are less sensitive than
methods using the combined category and size class distributions
(<em>gain3</em>, <em>ratio3</em>, <em>gini3</em>, <em>dist3</em>).

<p>
For a thorough change assessment it is recommended to calculate
different change assessment measures (at least information gain and
information gain ratio) and investigate the differences between these
change assessments.

<h2>NOTES</h2>
<h3>Shannon's entropy</h3>
Entropies for information gain and its ratio are by default Shannon
entropies <em>H</em>, calculated with<br><br>
H = <big>&sum;</big> p<sub>i</sub> * log<sub>2</sub>(p<sub>i</sub>)

<p>
The entropies are here calculated with base 2 logarithms. The upper
bound of information gain is thus log<sub>2</sub>(number of classes).
Classes can be categories, size classes, or unique combinations of
categories and size classes.

<h3>R&eacute;nyi's entropy</h3>
The <b>alpha</b> option can be used to calculate general entropies
<em>H<sub>&alpha;</sub></em> after R&eacute;nyi (1961) with the formula<br><br>
H<sub>&alpha;</sub> = 1 / (1 - &alpha;) * log<sub>2</sub>
(<big>&sum;</big> p<sub>i</sub><sup>&alpha;</sup>)

<p>
An <b>alpha</b> &lt; 1 gives higher weight to small frequencies,
whereas an <b>alpha</b> &gt; 1 gives higher weight to large
frequencies. This is useful for noisy input data such as the MODIS land
cover/land use products MCD12*. These data often differ only in
single-cell patches. These differences can be due to the applied
classification procedure. Moreover, the probabilities that a cell has
been assigned to class A or class B are often very similar, i.e.
different classes are confused by the applied classification procedure.
In such cases an <b>alpha</b> &gt; 1, e.g. 2, will give less weight to
small changes and more weight to large changes, to a degree alleviating
the problem of class confusion.

<h2>EXAMPLES</h2>

Assuming there is a time series of the MODIS land cover/land use
product MCD12Q1, land cover type 1, available, and the raster maps have
the names
<div class="code"><pre>
MCD12Q1.A2001.Land_Cover_Type_1
MCD12Q1.A2002.Land_Cover_Type_1
MCD12Q1.A2003.Land_Cover_Type_1
...
</pre></div>

then a change assessment can be done with
<div class="code"><pre>
r.change.info in=`g.list type=rast pat=MCD12Q1.A*.Land_Cover_Type_1 sep=,` \
              method=pc,gain1,gain2,ratio1,ratio2,dist1,dist2
              out=MCD12Q1.pc,MCD12Q1.gain1,MCD12Q1.gain2,MCD12Q1.ratio1,MCD12Q1.ratio2,MCD12Q1.dist1,MCD12Q1.dist2 \
              radius=20 step=40 alpha=2
</pre></div>

<h2>SEE ALSO</h2>

<em><a href="https://grass.osgeo.org/grass-stable/manuals/r.neighbors.html">r.neighbors</a></em><br>
<em><a href="https://grass.osgeo.org/grass-stable/manuals/g.region.html">g.region</a></em><br>
<em><a href="https://grass.osgeo.org/grass-stable/manuals/r.li.shannon.html">r.li.shannon</a></em><br>
<em><a href="https://grass.osgeo.org/grass-stable/manuals/r.li.simpson.html">r.li.simpson</a></em><br>
<em><a href="https://grass.osgeo.org/grass-stable/manuals/r.li.renyi.html">r.li.renyi</a></em><br>
<em><a href="https://grass.osgeo.org/grass-stable/manuals/r.clump.html">r.clump</a></em>

<h2>REFERENCES</h2>

<ul>
<li>
Quinlan, J.R. 1986. Induction of decision trees. Machine Learning 1: 81-106.
<a href="http://dx.doi.org/10.1007/BF00116251">DOI:10.1007/BF00116251</a>
<li>
R&eacute;nyi, A. 1961. <a href="http://digitalassets.lib.berkeley.edu/math/ucb/text/math_s4_v1_article-27.pdf">On measures of information and entropy.</a> Proceedings of the fourth Berkeley Symposium on Mathematics, Statistics and Probability 1960: 547-561.
<li>
Shannon, C.E. 1948. A Mathematical Theory of Communication. Bell System Technical Journal 27(3): 379-423. <a href="http://dx.doi.org/10.1002/j.1538-7305.1948.tb01338.x">DOI:10.1002/j.1538-7305.1948.tb01338.x</a>
</ul>

<h2>AUTHOR</h2>

Markus Metz
